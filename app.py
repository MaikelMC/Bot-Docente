from flask import Flask, render_template, request, jsonify
import os
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from config import VECTORSTORE_DIR, GEMINI_API_KEY, GROQ_API_KEY, EMBEDDING_MODEL, USE_GROQ

app = Flask(__name__)

# Configurar LLM seg√∫n la opci√≥n
if USE_GROQ:
    from groq import Groq
    llm_client = Groq(api_key=GROQ_API_KEY)
    print("‚úÖ Usando Groq API")
else:
    import google.generativeai as genai
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-pro')
    print("‚úÖ Usando Gemini API")

# Variables globales
embedding_model = None
faiss_index = None
chunks = None
system_info = None  # Nueva variable para almacenar info del sistema

def load_models():
    """Carga los modelos y datos necesarios"""
    global embedding_model, faiss_index, chunks, system_info
    
    print("üîÑ Cargando modelo de embeddings...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL)
    
    print("üîÑ Cargando vectorstore...")
    index_path = os.path.join(VECTORSTORE_DIR, "index.faiss")
    chunks_path = os.path.join(VECTORSTORE_DIR, "chunks.pkl")
    
    # Verificar que existan los archivos
    if not os.path.exists(index_path):
        raise FileNotFoundError(f"‚ùå No se encontr√≥ {index_path}. Ejecuta primero: python process_pdfs.py")
    
    if not os.path.exists(chunks_path):
        raise FileNotFoundError(f"‚ùå No se encontr√≥ {chunks_path}. Ejecuta primero: python process_pdfs.py")
    
    faiss_index = faiss.read_index(index_path)
    
    with open(chunks_path, 'rb') as f:
        chunks = pickle.load(f)
    
    # Verificar que se cargaron datos
    if faiss_index.ntotal == 0:
        raise ValueError("‚ùå El √≠ndice FAISS est√° vac√≠o!")
    
    if len(chunks) == 0:
        raise ValueError("‚ùå No hay chunks cargados!")
    
    print(f"‚úÖ Sistema listo:")
    print(f"   - Vectores FAISS: {faiss_index.ntotal}")
    print(f"   - Chunks de texto: {len(chunks)}")
    
    # Generar resumen del sistema
    system_info = generate_system_summary()

def generate_system_summary():
    """Genera un resumen de la informaci√≥n disponible"""
    sources = {}
    for chunk in chunks:
        source = chunk['source']
        if source not in sources:
            sources[source] = []
        sources[source].append(chunk['text'])
    
    # Crear resumen
    summary = {
        'total_chunks': len(chunks),
        'total_documents': len(sources),
        'documents': []
    }
    
    for source, texts in sources.items():
        # Tomar los primeros caracteres de varios chunks para hacer un preview
        preview_text = " ".join(texts[:3])[:500]
        summary['documents'].append({
            'name': source,
            'chunks': len(texts),
            'preview': preview_text
        })
    
    return summary

def search_similar_chunks(query, k=5):  # Aumentado a 5 para mejor contexto
    """Busca los chunks m√°s similares a la pregunta"""
    try:
        # Generar embedding de la pregunta
        query_embedding = embedding_model.encode([query])
        
        # Convertir a formato numpy float32
        query_vector = np.array(query_embedding).astype('float32')
        
        # Buscar en FAISS
        distances, indices = faiss_index.search(query_vector, k)
        
        # Obtener chunks relevantes
        relevant_chunks = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(chunks) and idx >= 0:
                chunk_with_score = chunks[idx].copy()
                chunk_with_score['similarity_score'] = float(distance)
                relevant_chunks.append(chunk_with_score)
        
        # Debug: mostrar scores
        print(f"   Similarity scores: {[f'{c['similarity_score']:.2f}' for c in relevant_chunks]}")
        
        return relevant_chunks
    
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda: {e}")
        raise

def generate_answer(question, context_chunks):
    """Genera respuesta usando LLM (Groq o Gemini)"""
    try:
        # Construir contexto
        context = "\n\n".join([
            f"[Fragmento {i+1} de {chunk['source']}]:\n{chunk['text']}" 
            for i, chunk in enumerate(context_chunks)
        ])
        
        # Crear prompt mejorado
        prompt = f"""Eres un asistente educativo experto. Responde la siguiente pregunta bas√°ndote √öNICAMENTE en el contexto proporcionado.

CONTEXTO DISPONIBLE:
{context}

PREGUNTA DEL ESTUDIANTE: {question}

INSTRUCCIONES:
- Responde de forma clara y detallada en espa√±ol
- Usa TODA la informaci√≥n relevante del contexto proporcionado
- Si encuentras informaci√≥n relacionada en el contexto, √∫sala aunque no sea una respuesta completa
- Solo di que no tienes informaci√≥n si REALMENTE no hay NADA relacionado en el contexto
- Cita las fuentes mencionando el nombre del documento
- S√© educativo y explica con detalle

RESPUESTA:"""
        
        if USE_GROQ:
            # Usar Groq con modelo correcto
            chat_completion = llm_client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
                model="moonshotai/kimi-k2-instruct-0905",  # CORREGIDO: Modelo v√°lido de Groq
                temperature=0.5,  # Aumentado para respuestas m√°s naturales
                max_tokens=1500,  # Aumentado para respuestas m√°s completas
            )
            return chat_completion.choices[0].message.content
        else:
            # Usar Gemini
            response = gemini_model.generate_content(prompt)
            return response.text
    
    except Exception as e:
        print(f"‚ùå Error generando respuesta: {e}")
        return f"Error al generar respuesta: {str(e)}"

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('index.html')

@app.route('/system-info')
def get_system_info():
    """Endpoint para obtener informaci√≥n del sistema"""
    if system_info is None:
        return jsonify({'error': 'Sistema no inicializado'}), 500
    
    return jsonify(system_info)

@app.route('/chat', methods=['POST'])
def chat():
    """Endpoint para procesar preguntas"""
    try:
        data = request.json
        question = data.get('question', '')
        
        if not question:
            return jsonify({'error': 'No se proporcion√≥ pregunta'}), 400
        
        print(f"\nüí¨ Pregunta recibida: {question}")
        
        # 1. Buscar chunks relevantes
        print("üîç Buscando informaci√≥n relevante...")
        relevant_chunks = search_similar_chunks(question, k=5)
        print(f"‚úì Encontrados {len(relevant_chunks)} fragmentos relevantes")
        
        # 2. Generar respuesta
        print("ü§ñ Generando respuesta...")
        answer = generate_answer(question, relevant_chunks)
        print("‚úì Respuesta generada")
        
        # 3. Preparar fuentes
        sources = list(set([chunk['source'] for chunk in relevant_chunks]))
        
        return jsonify({
            'answer': answer,
            'sources': sources,
            'debug': {
                'chunks_found': len(relevant_chunks),
                'similarity_scores': [chunk['similarity_score'] for chunk in relevant_chunks]
            }
        })
    
    except Exception as e:
        print(f"‚ùå Error en /chat: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # Cargar modelos al iniciar
    try:
        load_models()
    except Exception as e:
        print(f"\n‚ùå ERROR AL CARGAR SISTEMA: {e}")
        print("\n‚ö†Ô∏è  Aseg√∫rate de haber ejecutado: python process_pdfs.py")
        print("‚ö†Ô∏è  Y de tener PDFs en la carpeta data/pdfs/\n")
        exit(1)
    
    # Iniciar servidor
    print("\n" + "="*50)
    print("üöÄ Servidor iniciado en http://localhost:5000")
    print("="*50 + "\n")
    
    app.run(debug=True, port=5000, use_reloader=False)